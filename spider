#!/bin/bash

## Cleanup previous crawl
$(./cleanup-spider)

## Variables
seed='https://www.google.com'
log_file='log.txt'
output_file='output.txt'
url_file='urls.txt'
crawled='crawled.txt'
crawl_level=0

## Set up crawl
$(echo "$seed" > "$url_file")
: > "$crawled"

## Begin Crawling
while read url; do	
	## Check to see if page has already been crawled
	$(grep "^$url$" "$crawled" > /dev/null)
	
	## If it has not
	if [ $? -ne 0 ]; then
		echo "Crawling $url"

		## Download page, appending output to log file and output contents of page to output file
		$(wget -o "$log_file" -O "$output_file" "$url")
	
		## If the download was successful
		if [ $? -eq 0 ]; then
			## Parse the output file for links (following structure (http/https)://www.some-thing.com/)
			## Append found links to url file
			$(grep -Eo "(http|https)://[A-Za-z0-9\./-]+" "$output_file" >> "$url_file")
			
			directory_level="./$directory_level/level$crawl_level"
			mkdir -p "$directory_level"
			$(grep -Eo "(http|https)://[A-Za-z0-9\./-]+" "$output_file" > "$directory_level/links.txt")
			crawl_level=$(($crawl_level+1))
		fi
		
		## Once crawl has finished *remember* that page has been crawled
		echo "$url" >> "$crawled"
	fi
done < "$url_file"
